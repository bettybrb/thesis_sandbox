{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a555f9b3",
      "metadata": {},
      "source": [
        "## CLASSIFICATION (EEGNet notebook explained)\n",
        "\n",
        "# Compare classifier performance using:\n",
        "\n",
        "* Real training data\n",
        "* Generated training data\n",
        "\n",
        "STEP 1 — Load original training data\n",
        "- For every subject & every class 0..3:\n",
        "Read CSV\n",
        "Drop first column\n",
        "Keep 1000 samples\n",
        "Scale values to 0–1 (MinMax scaling)\n",
        "Append to train list\n",
        "- Do same for:\n",
        "Original/Axx/test/...\n",
        "- Now you have:\n",
        "train_dataset\n",
        "train_label\n",
        "test_dataset\n",
        "test_label\n",
        "\n",
        "STEP 2 — Load generated training data\n",
        "For every subject & every class:\n",
        "Read CSV\n",
        "Keep 1000 samples\n",
        "Scale values\n",
        "Append\n",
        "No test data from generated set.\n",
        "\n",
        "STEP 3 — Prepare shapes\n",
        "Neural networks expect 4D input:\n",
        "(samples, channels, time, 1)\n",
        "So you add one dimension at the end.\n",
        "Labels must be converted to one-hot:\n",
        "Class 2 → [0,0,1,0]\n",
        "\n",
        "STEP 4 — Define EEGNet\n",
        "EEGNet is a small CNN specialized for EEG.\n",
        "Conceptually it does:\n",
        "1. Temporal convolution\n",
        "Learns frequency/time patterns in each channel\n",
        "2. Depthwise spatial convolution\n",
        "Learns relationships between channels\n",
        "3. Pooling\n",
        "Reduces temporal resolution\n",
        "4. Separable convolution\n",
        "Learns more complex features efficiently\n",
        "5. Flatten\n",
        "Convert feature maps to vector\n",
        "6. Dense + Softmax\n",
        "Output class probabilities\n",
        "\n",
        "STEP 5 — Train two models\n",
        "Model 1:\n",
        "- Train on original training data.\n",
        "Model 2:\n",
        "- Train on generated training data.\n",
        "Both are evaluated on the same real test dataset.\n",
        "\n",
        "STEP 6 — Evaluation\n",
        "Predict labels on real test set.\n",
        "        Compute:\n",
        "        Precision\n",
        "        Recall\n",
        "        F1-score\n",
        "        Accuracy\n",
        "Compare original vs generated training performance.\n",
        "If generated improves performance → augmentation helped.\n",
        "\n",
        "MPORTANT CONSISTENCY RULE\n",
        "- Your trials must all have the same shape.\n",
        "- If you extract 1000 timepoints: Then EEGNet must use Samples = 1000.\n",
        "- If you choose 500: Then everything must consistently use 500."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5d5774a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PART 3: CLASSIFICATION (train EEGNet twice; evaluate on real test)\n",
        "# ============================================================\n",
        "\n",
        "def classification_experiment() -> None:\n",
        "    \"\"\"\n",
        "    Train and evaluate classifier twice:\n",
        "      A) Train on Original training set, test on Original test set\n",
        "      B) Train on Generated training set, test on Original test set\n",
        "    \"\"\"\n",
        "\n",
        "    # -----------------------\n",
        "    # Step 1: Load datasets\n",
        "    # -----------------------\n",
        "    X_train_orig, y_train_orig = load_dataset_tree(\n",
        "        root=ORIGINAL_ROOT, split=\"train\",\n",
        "        drop_first_column=True,             # original CSVs have channel-name column\n",
        "        expected_shape=(22, 1000),\n",
        "    )\n",
        "\n",
        "    X_test_real, y_test_real = load_dataset_tree(\n",
        "        root=ORIGINAL_ROOT, split=\"test\",\n",
        "        drop_first_column=True,\n",
        "        expected_shape=(22, 1000),\n",
        "    )\n",
        "\n",
        "    X_train_gen, y_train_gen = load_dataset_tree_generated(\n",
        "        root=GENERATED_ROOT, split=\"train\",\n",
        "        drop_first_column=False,            # generated CSVs should be numeric only\n",
        "        expected_shape=(22, 1000),\n",
        "    )\n",
        "\n",
        "    # -----------------------\n",
        "    # Step 2: Scale features\n",
        "    # -----------------------\n",
        "    # IMPORTANT: fit scaler on training data only, then apply to train+test.\n",
        "    scaler_orig = fit_minmax_scaler(X_train_orig)\n",
        "    X_train_orig = apply_scaler(X_train_orig, scaler_orig)\n",
        "    X_test_real_A = apply_scaler(X_test_real, scaler_orig)\n",
        "\n",
        "    scaler_gen = fit_minmax_scaler(X_train_gen)\n",
        "    X_train_gen = apply_scaler(X_train_gen, scaler_gen)\n",
        "    X_test_real_B = apply_scaler(X_test_real, scaler_gen)\n",
        "\n",
        "    # -----------------------\n",
        "    # Step 3: Shape formatting\n",
        "    # -----------------------\n",
        "    # Many CNN implementations expect (N, channels, samples, 1)\n",
        "    X_train_orig = add_last_dim(X_train_orig)  # -> (N, 22, 1000, 1)\n",
        "    X_train_gen = add_last_dim(X_train_gen)\n",
        "    X_test_real_A = add_last_dim(X_test_real_A)\n",
        "    X_test_real_B = add_last_dim(X_test_real_B)\n",
        "\n",
        "    # Convert integer labels 0..3 -> one-hot vectors\n",
        "    y_train_orig_oh = one_hot(y_train_orig, num_classes=4)\n",
        "    y_train_gen_oh = one_hot(y_train_gen, num_classes=4)\n",
        "\n",
        "    # -----------------------\n",
        "    # Step 4: Train model A (original)\n",
        "    # -----------------------\n",
        "    model_A = define_eegnet_like_model(num_classes=4, channels=22, samples=1000)\n",
        "    model_A = train_model(\n",
        "        model=model_A,\n",
        "        X=X_train_orig,\n",
        "        y=y_train_orig_oh,\n",
        "        validation_split=0.2,\n",
        "        early_stopping_metric=\"val_accuracy\",\n",
        "    )\n",
        "\n",
        "    # Evaluate on real test\n",
        "    yhat_A = predict_classes(model_A, X_test_real_A)\n",
        "    report_A = classification_report(y_true=y_test_real, y_pred=yhat_A)\n",
        "    print(\"Model A (trained on Original) performance:\")\n",
        "    print(report_A)\n",
        "\n",
        "    # -----------------------\n",
        "    # Step 5: Train model B (generated)\n",
        "    # -----------------------\n",
        "    model_B = define_eegnet_like_model(num_classes=4, channels=22, samples=1000)\n",
        "    model_B = train_model(\n",
        "        model=model_B,\n",
        "        X=X_train_gen,\n",
        "        y=y_train_gen_oh,\n",
        "        validation_split=0.2,\n",
        "        early_stopping_metric=\"val_accuracy\",\n",
        "    )\n",
        "\n",
        "    # Evaluate on real test\n",
        "    yhat_B = predict_classes(model_B, X_test_real_B)\n",
        "    report_B = classification_report(y_true=y_test_real, y_pred=yhat_B)\n",
        "    print(\"Model B (trained on Generated) performance:\")\n",
        "    print(report_B)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
